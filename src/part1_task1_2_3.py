# -*- coding: utf-8 -*-
"""Part1_Task1_2_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tStEjlOjs6_vsDTwg9NnC7-GyPHWIiHc

#Part 1: Spark RDD API

All data manipulation, aggregation, or ML applications must be implemented on top of Apache Spark. Hence we are importing pyspark in this case.
"""

pip install pyspark

"""##Importing any libraries that we might need


"""

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark import SparkContext
from operator import add
from pyspark.sql.functions import max,col,min,avg,split

"""##Task 1
Firstly we are going to create a few summaries and counts on grocery shopping items using the dataset groceries.csv

spark.read.csv() method can be used to read a single CSV and return a Spark dataframe.
"""

spark = SparkSession.builder.appName("answers").getOrCreate()
!wget --continue https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv -O /tmp/groceries.csv
path = "/tmp/groceries.csv"

#reading csv file and creating a dataframe
groceries_df = spark.read.csv(path)
groceries_df.printSchema()

#printing out the dataframe
groceries_df.show()

#reorganising the dataframe to be a single column 
groceries_union = groceries_df.select("_c0").union(groceries_df.select("_c1"))
groceries_union = groceries_union.select("_c0").union(groceries_df.select("_c2"))
groceries_union = groceries_union.select("_c0").union(groceries_df.select("_c3"))

groceries_union.show()

"""##Task 2a

 Creating a list of all (unique) products present in the transactions. Doing the above using distinct(). Writing this list to out_1_2a.txt
"""

groceries_unique_list_df = groceries_union.distinct()
groceries_unique_list_df.write.text("/content/drive/MyDrive/out_1_2a.txt")

"""##Task 2b

Total count of products to a text file called out_1_2b.txt.

The format should be:

Count:

*total count*
"""

groceries_unique_count = groceries_unique_list_df.count()
limit = 'Count: ' + '\n' + str(groceries_unique_count) #formatting for file 

#creating a dataframe in order to write it to text file
groceries_unique_count_df = spark.createDataFrame(spark.sparkContext.parallelize([limit]),StringType()).coalesce(1).write.format("text").mode("overwrite").save("/content/drive/MyDrive/out_1_2b.txt")
#spark.stop()

"""##Task 3
Determining the top 5 purchased products along with how often they were purchased (frequency count).

Ordering them in descending order and limiting it to the first 5 products.
"""

#groceries_union.write.text("/content/drive/MyDrive/groceries_union.txt")
lines = spark.sparkContext.textFile("/content/drive/MyDrive/groceries_union.txt")

# counts is an rdd is of the form (word, count)
counts = lines.flatMap(lambda x: x.split(',')).map(lambda x: (x, 1)).reduceByKey(add)


# collect brings it to a list 
output = counts.collect()
list_of_freq_count = []
for (word, count) in output:
    #print("%s: %i" % (word, count))
    list_of_freq_count.append([word, count])
    
list_of_freq_count_df = spark.createDataFrame(list_of_freq_count)
list_of_freq_count_df = list_of_freq_count_df.withColumnRenamed('_1','Food name')
list_of_freq_count_df = list_of_freq_count_df.withColumnRenamed('_2','Frequency_Count')
list_of_top_5_freq_count_df = list_of_freq_count_df.orderBy(list_of_freq_count_df.Frequency_Count.desc())
list_of_top_5_freq_count_df = list_of_top_5_freq_count_df.limit(5)
list_of_top_5_freq_count_df.write.csv("/content/drive/MyDrive/out_1_3.txt")



